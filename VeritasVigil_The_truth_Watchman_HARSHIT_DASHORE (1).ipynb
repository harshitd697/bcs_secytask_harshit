{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "PZzfitQ32KgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import uuid"
      ],
      "metadata": {
        "id": "zKbkQ5gvz2OM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customize tokenizer"
      ],
      "metadata": {
        "id": "vdBXHBX_2Yhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_repeats(text):\n",
        "\n",
        "    def replace_repeat(match):\n",
        "        char = match.group(1)\n",
        "        count = len(match.group(0))\n",
        "        return f\"{char} <REPEAT:{count}>\"\n",
        "    return re.sub(r'([a-zA-Z])\\1{2,}', replace_repeat, text)"
      ],
      "metadata": {
        "id": "mTkRzzMyz3SE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_contractions(text):\n",
        "    contractions = {\n",
        "        \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\",\n",
        "        \"won't\": \"will not\", \"can't\": \"cannot\", \"'re\": \"are\",\n",
        "        \"'ll\": \"will\", \"'ve\": \"have\", \"'d\": \"would\"\n",
        "    }\n",
        "    for contraction, expanded in contractions.items():\n",
        "        text = text.replace(contraction, expanded)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Ri1Xm476z_MM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = handle_contractions(text)\n",
        "    text = normalize_repeats(text)\n",
        "    tokens = re.findall(r'\\b\\w+\\b|[:;][-]?[)D(PO]|<REPEAT:\\d+>', text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Hg7sNPnQz_P5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rule based POS Tagger"
      ],
      "metadata": {
        "id": "8XvVfJcF2jTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_pos(token):\n",
        "    noun_suffixes = ['tion', 'sion', 'ity', 'ment', 'ness', 'ance', 'ence']\n",
        "    verb_suffixes = ['ize', 'ise', 'ed', 'ing', 'ate']\n",
        "    adj_suffixes = ['al', 'ive', 'ous', 'ful', 'less', 'able']\n",
        "\n",
        "    if any(token.endswith(suffix) for suffix in noun_suffixes):\n",
        "        return 'NOUN'\n",
        "    elif any(token.endswith(suffix) for suffix in verb_suffixes):\n",
        "        return 'VERB'\n",
        "    elif any(token.endswith(suffix) for suffix in adj_suffixes):\n",
        "        return 'ADJ'\n",
        "    elif re.match(r'[:;][-]?[)D(PO]', token):\n",
        "        return 'EMOTICON'\n",
        "    elif token.startswith('<REPEAT:'):\n",
        "        return 'REPEAT'\n",
        "    return 'OTHER'"
      ],
      "metadata": {
        "id": "h2VpKFVvz_Rr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "customize lemmatizer"
      ],
      "metadata": {
        "id": "3kI9p3q32vz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(token):\n",
        "    pos = tag_pos(token)\n",
        "    verb_rules = {\n",
        "        'ing': lambda x: x[:-3] if x.endswith('ing') else x,\n",
        "        'ed': lambda x: x[:-2] if x.endswith('ed') else x,\n",
        "        'es': lambda x: x[:-2] if x.endswith('es') else x,\n",
        "        's': lambda x: x[:-1] if x.endswith('s') else x\n",
        "    }\n",
        "    noun_rules = {\n",
        "        's': lambda x: x[:-1] if x.endswith('s') else x,\n",
        "        'es': lambda x: x[:-2] if x.endswith('es') else x\n",
        "    }\n",
        "    adj_rules = {\n",
        "        'er': lambda x: x[:-2] if x.endswith('er') else x,\n",
        "        'est': lambda x: x[:-3] if x.endswith('est') else x\n",
        "    }\n",
        "\n",
        "    if pos == 'VERB':\n",
        "        for suffix, rule in verb_rules.items():\n",
        "            if token.endswith(suffix):\n",
        "                return rule(token)\n",
        "    elif pos == 'NOUN':\n",
        "        for suffix, rule in noun_rules.items():\n",
        "            if token.endswith(suffix):\n",
        "                return rule(token)\n",
        "    elif pos == 'ADJ':\n",
        "        for suffix, rule in adj_rules.items():\n",
        "            if token.endswith(suffix):\n",
        "                return rule(token)\n",
        "    return token"
      ],
      "metadata": {
        "id": "C3h22dKkz_Ty"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature extraction"
      ],
      "metadata": {
        "id": "XgD_9Xxw22pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_bow(documents):\n",
        "    word_counts = Counter()\n",
        "    for doc in documents:\n",
        "        word_counts.update(set(doc))\n",
        "    vocab = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "sgE5eNBjz_Vw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_bow(documents, vocab):\n",
        "    bow_matrix = np.zeros((len(documents), len(vocab)))\n",
        "    for i, doc in enumerate(documents):\n",
        "        for word in doc:\n",
        "            if word in vocab:\n",
        "                bow_matrix[i, vocab[word]] += 1\n",
        "    return bow_matrix"
      ],
      "metadata": {
        "id": "pGyx4Gtkz_Xs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_tfidf(documents):\n",
        "    vocab = fit_bow(documents)\n",
        "    doc_count = len(documents)\n",
        "    idf = {word: np.log(doc_count / (1 + sum(word in doc for doc in documents))) for word in vocab}\n",
        "    return vocab, idf\n"
      ],
      "metadata": {
        "id": "bzpB3yy6z_Zk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_tfidf(documents, vocab, idf):\n",
        "    bow_matrix = transform_bow(documents, vocab)\n",
        "    tfidf_matrix = np.zeros_like(bow_matrix)\n",
        "    for word, j in vocab.items():\n",
        "        tfidf_matrix[:, j] = bow_matrix[:, j] * idf.get(word, 0)\n",
        "    return tfidf_matrix\n"
      ],
      "metadata": {
        "id": "PtJ6q8iUz_bg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visuliations"
      ],
      "metadata": {
        "id": "64ia3iAU27VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_token_frequency(tokens, title, filename):\n",
        "    freq = Counter(tokens)\n",
        "    top_tokens = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar([t[0] for t in top_tokens], [t[1] for t in top_tokens])\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "y9j7KvFpz_dg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_repeat_distribution(tokens, filename):\n",
        "    repeat_tokens = [t for t in tokens if t.startswith('<REPEAT:')]\n",
        "    repeat_counts = [int(re.search(r'<REPEAT:(\\d+)>', t).group(1)) for t in repeat_tokens if re.search(r'<REPEAT:(\\d+)>', t)]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if repeat_counts:\n",
        "        plt.hist(repeat_counts, bins=range(1, max(repeat_counts)+2))\n",
        "        plt.xlabel('Repeat Count')\n",
        "        plt.ylabel('Frequency')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No repeat tokens found', horizontalalignment='center', verticalalignment='center')\n",
        "    plt.title('Distribution of Repeated Character Counts')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "rMcq2eO_0pRv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, filename):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "ZF4G8Vce0pVF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN"
      ],
      "metadata": {
        "id": "PMIGtO6P3EDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df = pd.read_csv('Fake.csv')\n",
        "true_df = pd.read_csv('True.csv')\n"
      ],
      "metadata": {
        "id": "RA8R1Z5A0pdm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df['label'] = 0\n",
        "true_df['label'] = 1\n",
        "df = pd.concat([fake_df[['title', 'label']], true_df[['title', 'label']]], ignore_index=True)\n",
        "headlines = df['title'].tolist()\n",
        "labels = df['label'].tolist()"
      ],
      "metadata": {
        "id": "lFVwJSLF1AgY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs = [tokenize(headline) for headline in headlines]\n",
        "lemmatized_docs = [[lemmatize(token) for token in doc] for doc in tokenized_docs]"
      ],
      "metadata": {
        "id": "VFbcZ-Ut1AiS"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    X_train, X_test, y_train, y_test = train_test_split(lemmatized_docs, labels, test_size=0.2, random_state=42)\n",
        "    vocab, idf = fit_tfidf(X_train)\n",
        "    X_train_tfidf = transform_tfidf(X_train, vocab, idf)\n",
        "    X_test_tfidf = transform_tfidf(X_test, vocab, idf)"
      ],
      "metadata": {
        "id": "O9ydMady1I-k"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    results = {}\n",
        "    all_tokens = [token for doc in tokenized_docs for token in doc]"
      ],
      "metadata": {
        "id": "9B7yaxp_1AkQ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "ANkuRiZU3Ivp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    nb_clf = MultinomialNB()\n",
        "    nb_clf.fit(X_train_tfidf, y_train)\n",
        "    nb_pred = nb_clf.predict(X_test_tfidf)\n",
        "    nb_scores = nb_clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "    results['Naive Bayes'] = {\n",
        "        'accuracy': accuracy_score(y_test, nb_pred),\n",
        "        'precision': precision_score(y_test, nb_pred),\n",
        "        'recall': recall_score(y_test, nb_pred),\n",
        "        'f1': f1_score(y_test, nb_pred)\n",
        "    }\n",
        "    plot_confusion_matrix(y_test, nb_pred, 'cm_naive_bayes.png')\n",
       
      ],
      "metadata": {
        "id": "QZ_kXFfg1AmN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "9jPnqFO43ORT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    svm_clf = LinearSVC()\n",
        "    svm_clf.fit(X_train_tfidf, y_train)\n",
        "    svm_pred = svm_clf.predict(X_test_tfidf)\n",
        "    svm_scores = svm_clf.decision_function(X_test_tfidf)\n",
        "    results['SVM'] = {\n",
        "        'accuracy': accuracy_score(y_test, svm_pred),\n",
        "        'precision': precision_score(y_test, svm_pred),\n",
        "        'recall': recall_score(y_test, svm_pred),\n",
        "        'f1': f1_score(y_test, svm_pred)\n",
        "    }\n",
        "    plot_confusion_matrix(y_test, svm_pred, 'cm_svm.png')\n"
    
      ],
      "metadata": {
        "id": "aewk342Q1Ao2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualisation"
      ],
      "metadata": {
        "id": "mVlLo-CO3fn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_token_frequency(all_tokens, 'Token Frequency Distribution', 'token_freq.png')\n",
        "plot_word_cloud([token for doc in lemmatized_docs for token in doc], 'wordcloud.png')\n",
        "plot_repeat_distribution(all_tokens, 'repeat_dist.png')"
      ],
      "metadata": {
        "id": "cbiqNqzp1AyR"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print performance"
      ],
      "metadata": {
        "id": "nD1HDj6X3jHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classifier Performance:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H6B2tyV1ZlF",
        "outputId": "42f320a4-d101-42c3-efe0-a97e65cc5c03"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Performance:\n",
            "\n",
            "Naive Bayes:\n",
            "Accuracy: 0.9395\n",
            "Precision: 0.9517\n",
            "Recall: 0.9188\n",
            "F1: 0.9349\n",
            "\n",
            "SVM:\n",
            "Accuracy: 0.9531\n",
            "Precision: 0.9461\n",
            "Recall: 0.9553\n",
            "F1: 0.9507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('report.txt', 'w') as f:\n",
        "        f.write(\"Custom News Classifier Report\\n\")\n",
        "        f.write(\"=\"*30 + \"\\n\")\n",
        "        f.write(\"Dataset: Fake and Real News Dataset (Kaggle)\\n\")\n",
        "        f.write(\"Tokenizer:\\n\")\n",
        "        f.write(\"- Expands contractions (e.g., isn't → is not)\\n\")\n",
        "        f.write(\"- Normalizes repeats (e.g., sooooo → so <REPEAT:5>)\\n\")\n",
        "        f.write(\"- Preserves emoticons (e.g., :D)\\n\")\n",
        "        f.write(\"POS Tagger:\\n\")\n",
        "        f.write(\"- Uses suffix rules for nouns (-tion), verbs (-ing), adjectives (-ous)\\n\")\n",
        "        f.write(\"Lemmatizer:\\n\")\n",
        "        f.write(\"- Applies POS-based rules (e.g., running → run for verbs)\\n\")\n",
        "        f.write(\"Performance:\\n\")\n",
        "        for name, metrics in results.items():\n",
        "            f.write(f\"\\n{name}:\\n\")\n",
        "            for metric, value in metrics.items():\n",
        "                f.write(f\"  {metric.capitalize()}: {value:.4f}\\n\")\n",
        "        f.write(\"\\nComparison with Off-the-Shelf:\\n\")\n",
        "        f.write(\"- Custom tokenizer handles repeats better\\n\")\n",
        "        f.write(\"- Rule-based POS/lemmatization is simple but less robust than spaCy\\n\")"
      ],
      "metadata": {
        "id": "SSdtjtoa1Zm3"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aAqVkKsZMIjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
